{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'clean'\n",
    "DATA_PATH_X = 'clean/x-data'\n",
    "DATA_PATH_Y = 'clean/y-data'\n",
    "CODES_PATH = \"clean/codes.txt\"\n",
    "BATCH_SIZE = 4\n",
    "IMG_SIZE = (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None):\n",
    "        self.label_dir = label_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = f\"{idx}.png\"\n",
    "        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n",
    "        label_path = os.path.join(self.label_dir, os.listdir(self.label_dir)[idx])\n",
    "        image = read_image(img_path)\n",
    "        label = read_image(label_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __init__(self, img_size):\n",
    "        self.img_size = (img_size, img_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        resized_img = transforms.Resize(size=self.img_size)(x)\n",
    "        pil_img =transforms.ToPILImage()(resized_img)\n",
    "        gray_img = transforms.functional.to_grayscale(pil_img, num_output_channels=1)\n",
    "        return transforms.ToTensor()(gray_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = CustomImageDataset(DATA_PATH_X, DATA_PATH_Y, transform=MyTransform(IMG_SIZE))\n",
    "dls =  torch.utils.data.DataLoader(dset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([4, 1, 128, 128])\n",
      "Shape of y: torch.Size([4, 1, 128, 128]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "for X, y in dls:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the parent constructor\n",
    "        super(LSNet, self).__init__()\n",
    "        self.conv_part = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=11, stride=4, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.deconv_part = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(64, 32, kernel_size=11, stride=4, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            torch.nn.ConvTranspose2d(32, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            torch.nn.ConvTranspose2d(16, 1, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Upsample(size=IMG_SIZE, mode='bilinear'),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_part(x)\n",
    "        x = self.deconv_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 128, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = torch.rand(20, 1, IMG_SIZE, IMG_SIZE)\n",
    "print(input_test.shape)\n",
    "model(input_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.454311  [    0/  858]\n",
      "loss: 0.038451  [  400/  858]\n",
      "loss: 0.026869  [  800/  858]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.073936  [    0/  858]\n",
      "loss: 0.025417  [  400/  858]\n",
      "loss: 0.025735  [  800/  858]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.059718  [    0/  858]\n",
      "loss: 0.021228  [  400/  858]\n",
      "loss: 0.021297  [  800/  858]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.053381  [    0/  858]\n",
      "loss: 0.017154  [  400/  858]\n",
      "loss: 0.023573  [  800/  858]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.053134  [    0/  858]\n",
      "loss: 0.015619  [  400/  858]\n",
      "loss: 0.022245  [  800/  858]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dls, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/main/model_2022_07_21_15_51.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128])\n",
      "torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x, y = dset[20][0], dset[20][1]\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "with torch.no_grad():\n",
    "    pred = model(x[None,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = pred[0]\n",
    "new_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x21508B69208>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAES0lEQVR4nO2bv28cRRTHP/au7fPezvriRIlsCySQ+RkkKqADo8glKHJpUdG6QEiIggr+ARpEHaWhIUrSISUFFlAgkQIJIWSlPd8lwpGJ73KO7yfF7t6d1ybeNzd7kyz+VrO789587703b97M7k30MIXNj3h7aZuf+X5FIDVhhsClF1+AKlB/CNSA2yklXQOjb/7B5Zqu8OgEVtfrUWuyC/7DMRP4YG2933Y8QNEYI4H3N9ailgIaHQA8OgIVk6OMf20j1OH7fgG4oKNjBAusrwGz+6jJrr6SlATmr6/EzU1gBeDaGoCroBhNAUcBgTAG0uWBVWBX/QifLN8tQwX4DIAzhSpQg9I/sPuYMBnsAVdTEkjvgjOs8uEyL5Xhwn348yKwED4qztEAmsCj1PoiaAWhA/wAhTkAnntlDupPlvhvpLOAc9zEUnSB2ecBaAPuWagWeZABAQ+geZAYH8APwqvzA0Vna4vGCQAwPX30Xjj8xAKVxINf0mqVx4ATjfsVdNoALC4c7XUprbp0FlCNw0Hg70GXCsxDYT66ue949BQ0nA58aZYAHgQc7FDtsbwFLEJkdRXGwnmXLQ/uAZNe2sEFBACYWSonPeZ4ANPnmjuDey3grmECXpReWzPAuf5gD5oe4DmwkxC4YpiAG022XgeYWQqTbRmo7cFb4bMtx6PiQ7chWo5lq2G7PXRRKcF3L0PglSDOC9AQhcAo9UC8CN9p3EtWhK+m16JZDwRxow789Vp00c9Gq5kTOIRHdwpvNONitMq0+7tA2AQBeLylBhfN5ucC0ZFqwgSaceN1gZDQAj6h2/34RiW88S6EyeBvmT5TLuAnP1qC28CnEkmZC6ZS9ZrNjkAGkLmg1YoadWBqJrrYgyBqy/fahmJgm7BcqgG3vpBImnLBMUXReAkM4WNR7xFc0A+IBDbGReAQqmGNID8osT4NMyCwbZvATVFvUzHQLrHAUxIDl60QKA2aS1YIaMNQDKjop6iTOh6BOQsU9cTMuyB5UjB2Ar/KupvKA3Uq6KQB+7PALAGXoV2bDQJwKCWNl4CDsCA3TUBjcLMENDWZmIbFnvCIfhgmLOC4LkAQBIFSSn07dgIjIQMC39gmIMMpgdwQ2B803xEJmqoHpnwU4eHVLZGgKQv4J3fJlsCQJWXnA3kJwuEySHZAYc4CE3piOSJQBA5O7HUEZvcFVs8HNDNBXqahviZTMRDB5vlAH9aLUuvnhNdFvQ3GQLweHPOhxROQo2l4SuCUQB/v2SYgg6k84Bb6zV2RYAYW+M02ARlMERh4YHwvLpNQPCXvjGxtTDSRm5qwHr47liNHLnhWCZjLA129b5utW8D6NLT+/UAed0Y3bBMo2yYgg8F6IP6oX/Ye37oF8kjg/3lYPYzTmtAWAU3kiIDsfw0ZENCEubXgfge7e8Nn9RsS6zWhqRho7ff9b70m/NoKgaG/PrxphQDSY/oY/wI0JcUAhEeK/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x21508B695C8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef2e8a0485ff57ee407a25f71772b040025efc0ca0a35f1571a51591b5255e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
